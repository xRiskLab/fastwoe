{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a0f5ca",
   "metadata": {},
   "source": [
    "# Weight of Evidence (WOE) and Standard Errors\n",
    "\n",
    "Author: https://www.github.com/deburky\n",
    "\n",
    "This notebook demonstrates the relationship between Weight of Evidence (WOE), log odds, and their standard errors. We will show that:\n",
    "\n",
    "1. **WOE is a centered version of log odds** - subtracting the prior log odds\n",
    "2. **WOE and log odds have identical standard errors** - because subtracting a constant doesn't affect variance\n",
    "3. **Standard errors can be calculated from contingency tables** and match logistic regression results\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "### Key Definitions:\n",
    "- **Log odds for group**: $\\theta_1 = \\log\\left(\\frac{n_{11}}{n_{10}}\\right)$\n",
    "- **Prior log odds**: $\\theta_{\\text{prior}} = \\log\\left(\\frac{n_{\\text{pos}}}{n_{\\text{neg}}}\\right)$  \n",
    "- **Weight of Evidence**: $\\text{WOE}_1 = \\theta_1 - \\theta_{\\text{prior}}$\n",
    "\n",
    "### Standard Error Properties:\n",
    "- **SE of log odds**: $\\text{SE}(\\theta_1) = \\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{10}}}$\n",
    "- **SE of WOE**: $\\text{SE}(\\text{WOE}_1) = \\text{SE}(\\theta_1)$ (constant subtraction doesn't change variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42f6dd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Sample Data Setup\n",
    "\n",
    "We'll work with a simple 2x2 contingency table to demonstrate the concepts:\n",
    "\n",
    "| Color | Good (y=0) | Bad (y=1) | Total |\n",
    "|-------|------------|-----------|-------|\n",
    "| Red   | 10         | 30        | 40    |\n",
    "| Blue  | 20         | 10        | 30    |\n",
    "| Total | 30         | 40        | 70    |\n",
    "\n",
    "From this table we can calculate:\n",
    "- **Event rates**: Red = 30/40 = 0.75, Blue = 10/30 = 0.333\n",
    "- **Prior rate**: 40/70 = 0.571\n",
    "- **Log odds**: Red = ln(30/10) = 1.099, Blue = ln(10/20) = -0.693\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8013aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from fisher_scoring import LogisticRegression\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "from scipy.special import logit\n",
    "\n",
    "from fastwoe import FastWoe\n",
    "\n",
    "# Create a rich Console for Jupyter notebook\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the contingency table\n",
    "data = {\"Color\": [\"Red\", \"Blue\"], \"Good\": [10, 20], \"Bad\": [30, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Contingency Table:\")\n",
    "table = Table(title=\"Contingency Table\")\n",
    "table.add_column(\"Color\", justify=\"center\")\n",
    "table.add_column(\"Good\", justify=\"center\")\n",
    "table.add_column(\"Bad\", justify=\"center\")\n",
    "table.add_column(\"Total\", justify=\"center\")\n",
    "\n",
    "table.add_row(\"Red\", \"10\", \"30\", \"40\")\n",
    "table.add_row(\"Blue\", \"20\", \"10\", \"30\")\n",
    "table.add_row(\"Total\", \"30\", \"40\", \"70\")\n",
    "console.print(table)\n",
    "\n",
    "# Convert to individual observations (Bernoulli format)\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    # Add 'Bad' observations (Target=1)\n",
    "    rows += [{\"Color\": row[\"Color\"], \"Target\": 1}] * row[\"Bad\"]\n",
    "    # Add 'Good' observations (Target=0)\n",
    "    rows += [{\"Color\": row[\"Color\"], \"Target\": 0}] * row[\"Good\"]\n",
    "full_df = pd.DataFrame(rows)\n",
    "\n",
    "# Binary encode Color: Red=0, Blue=1\n",
    "full_df[\"Color_bin\"] = (full_df[\"Color\"] == \"Blue\").astype(int)\n",
    "\n",
    "print(f\"\\nConverted to {len(full_df)} individual observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27b8e8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Maximum Likelihood Logistic Regression\n",
    "\n",
    "Let's fit a logistic regression model and examine the standard errors. The model will be:\n",
    "$$\\text{logit}(P(\\text{Target}=1)) = \\beta_0 + \\beta_1 \\cdot \\text{Color\\_bin}$$\n",
    "\n",
    "Where Color_bin = 0 for Red and 1 for Blue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression using statsmodels\n",
    "X = sm.add_constant(full_df[\"Color_bin\"])\n",
    "y = full_df[\"Target\"]\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit(disp=False)\n",
    "\n",
    "print(\"Statsmodels Logistic Regression Results:\")\n",
    "print(f\"Intercept (β₀): {result.params['const']:.4f} (SE: {result.bse['const']:.4f})\")\n",
    "print(\n",
    "    f\"Color_bin (β₁): {result.params['Color_bin']:.4f} (SE: {result.bse['Color_bin']:.4f})\"\n",
    ")\n",
    "\n",
    "# Fit using Fisher Scoring implementation\n",
    "x = full_df[[\"Color_bin\"]]\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(x, y)\n",
    "logistic.display_summary(style=\"cyan1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff06bf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Interpretation of Results:\n",
    "\n",
    "1. **Intercept (1.0986)**: This is the log odds for the reference group (Red, Color_bin=0)\n",
    "   - Log odds = ln(30/10) = ln(3) ≈ 1.099 ✓\n",
    "\n",
    "2. **Coefficient (-1.7918)**: This is the difference in log odds between Blue and Red\n",
    "   - Blue log odds: ln(10/20) = ln(0.5) ≈ -0.693\n",
    "   - Red log odds: ln(30/10) = ln(3) ≈ 1.099  \n",
    "   - Difference: -0.693 - 1.099 = -1.792 ≈ -1.7918 ✓\n",
    "\n",
    "3. **Standard Errors**: These match the theoretical formulas from contingency table analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b191163",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Weight of Evidence (WOE) - Inference with Likelihood Ratios\n",
    "\n",
    "Now let's calculate WOE values and examine their standard errors. WOE transforms each group's log odds by subtracting the overall prior log odds:\n",
    "\n",
    "$$\n",
    "WOE = \\log \\left( \\frac{P(y=1|x)}{P(y=0|x)} \\right) - \\log \\left( \\frac{P(y=1)}{P(y=0)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WOE encoder\n",
    "fastwoe_encoder = FastWoe()\n",
    "fastwoe_encoder.fit(x, y)\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"WOE Feature Statistics:\")\n",
    "feature_stats = fastwoe_encoder.get_feature_stats()\n",
    "display(feature_stats)\n",
    "\n",
    "print(f\"\\nOverall prior probability: {fastwoe_encoder.y_prior_:.4f}\")\n",
    "print(\n",
    "    f\"Prior log odds: {np.log(fastwoe_encoder.y_prior_ / (1 - fastwoe_encoder.y_prior_)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WOE Mappings for Color_bin:\")\n",
    "woe_mappings = fastwoe_encoder.mappings_[\"Color_bin\"]\n",
    "display(woe_mappings)\n",
    "\n",
    "print(\"\\nKey WOE Values:\")\n",
    "print(\n",
    "    f\"Red (category 0): WOE = {woe_mappings.loc[0, 'woe']:.4f}, SE = {woe_mappings.loc[0, 'woe_se']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Blue (category 1): WOE = {woe_mappings.loc[1, 'woe']:.4f}, SE = {woe_mappings.loc[1, 'woe_se']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a785f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Manual Verification of WOE Calculations:\n",
    "\n",
    "Let's verify these WOE values manually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log odds for each group\n",
    "red_log_odds = np.log(30 / 10)  # 30 bad, 10 good for Red\n",
    "blue_log_odds = np.log(10 / 20)  # 10 bad, 20 good for Blue\n",
    "prior_log_odds = logit(40 / 70)  # 40 bad out of 70 total\n",
    "\n",
    "print(\"Manual Calculation Verification:\")\n",
    "print(f\"Red log odds: {red_log_odds:.4f}\")\n",
    "print(f\"Blue log odds: {blue_log_odds:.4f}\")\n",
    "print(f\"Prior log odds: {prior_log_odds:.4f}\")\n",
    "\n",
    "print(\"\\nManual WOE calculations:\")\n",
    "red_woe = red_log_odds - prior_log_odds\n",
    "blue_woe = blue_log_odds - prior_log_odds\n",
    "print(f\"Red WOE: {red_log_odds:.4f} - {prior_log_odds:.4f} = {red_woe:.4f}\")\n",
    "print(f\"Blue WOE: {blue_log_odds:.4f} - {prior_log_odds:.4f} = {blue_woe:.4f}\")\n",
    "\n",
    "print(\"\\nComparison with FastWoe:\")\n",
    "print(f\"Red: Manual={red_woe:.4f}, FastWoe={woe_mappings.loc[0, 'woe']:.4f}\")\n",
    "print(f\"Blue: Manual={blue_woe:.4f}, FastWoe={woe_mappings.loc[1, 'woe']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c178b57",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Standard Error Verification\n",
    "\n",
    "Now let's verify that the standard errors from the contingency table match those from logistic regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "red_bad, red_good = 30, 10\n",
    "blue_bad, blue_good = 10, 20\n",
    "\n",
    "red_se = (1 / red_bad + 1 / red_good) ** 0.5\n",
    "blue_se = (1 / blue_bad + 1 / blue_good) ** 0.5\n",
    "diff_se = (red_se**2 + blue_se**2) ** 0.5\n",
    "\n",
    "logistic_0 = LogisticRegression()\n",
    "logistic_0.fit(x, y)\n",
    "logistic_0_se = logistic_0.summary()[\"standard_errors\"][0]\n",
    "\n",
    "logistic_1 = LogisticRegression()\n",
    "logistic_1.fit(1 - x, y)\n",
    "logistic_1_se = logistic_1.summary()[\"standard_errors\"][0]\n",
    "\n",
    "woe_se_red = red_se\n",
    "woe_se_blue = blue_se\n",
    "\n",
    "se_table = Table(\n",
    "    title=\"WOE & Logistic Regression Standard Error Calculation\",\n",
    "    show_lines=True,\n",
    "    min_width=60,\n",
    ")\n",
    "se_table.add_column(\"Group\", style=\"bold cyan\", justify=\"center\")\n",
    "se_table.add_column(\"Bad\", justify=\"right\")\n",
    "se_table.add_column(\"Good\", justify=\"right\")\n",
    "se_table.add_column(\"SE (Table)\", justify=\"right\")\n",
    "se_table.add_column(\"SE (WOE)\", justify=\"right\")\n",
    "se_table.add_column(\"SE (Logistic)\", justify=\"right\")\n",
    "\n",
    "se_table.add_row(\n",
    "    \"Red\",\n",
    "    str(red_bad),\n",
    "    str(red_good),\n",
    "    f\"{red_se:.4f}\",\n",
    "    f\"{woe_se_red:.4f}\",\n",
    "    f\"{logistic_0_se:.4f}\",\n",
    ")\n",
    "se_table.add_row(\n",
    "    \"Blue\",\n",
    "    str(blue_bad),\n",
    "    str(blue_good),\n",
    "    f\"{blue_se:.4f}\",\n",
    "    f\"{woe_se_blue:.4f}\",\n",
    "    f\"{logistic_1_se:.4f}\",\n",
    ")\n",
    "se_table.add_row(\n",
    "    \"Difference\",\n",
    "    \"-\",\n",
    "    \"-\",\n",
    "    f\"{diff_se:.4f}\",\n",
    "    f\"{np.sqrt(red_se**2 + blue_se**2):.4f}\",\n",
    "    f\"{np.sqrt(logistic_0_se**2 + logistic_1_se**2):.4f}\",\n",
    ")\n",
    "\n",
    "# Center the table inside the panel with padding\n",
    "panel = Panel(\n",
    "    se_table,\n",
    "    title=\"WOE Standard Error Verification\",\n",
    "    subtitle=\"All standard errors should match.\",\n",
    "    expand=False,\n",
    "    padding=(1, 8),  # Top/bottom, left/right padding\n",
    ")\n",
    "\n",
    "console.print(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3500a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. WOE-Based Logistic Regression\n",
    "\n",
    "Now let's fit a logistic regression using WOE-transformed features and compare the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce871773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features using WOE\n",
    "X_woe = fastwoe_encoder.transform(x)\n",
    "print(\"WOE-transformed data (first 10 rows):\")\n",
    "print(pd.DataFrame(X_woe, columns=[\"Color_bin\"]).sample(10))\n",
    "\n",
    "# # Fit logistic regression on WOE-transformed data\n",
    "logistic_woe = LogisticRegression(use_bias=True, max_iter=10)\n",
    "logistic_woe.fit(X_woe, y)\n",
    "print(\"\\nWOE-based Logistic Regression:\")\n",
    "logistic_woe.display_summary(style=\"cyan1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_coef = logistic_woe.summary()[\"betas\"][1]\n",
    "woe_intercept = logistic_woe.summary()[\"betas\"][0]\n",
    "prior_log_odds = prior_log_odds  # already computed\n",
    "\n",
    "# Calculate match for intercept\n",
    "intercept_match = abs(woe_intercept - prior_log_odds) < 0.01\n",
    "\n",
    "# Make the table\n",
    "check_table = Table(\n",
    "    title=\"WOE vs Logistic Regression Verification\", show_lines=True, min_width=60\n",
    ")\n",
    "check_table.add_column(\"Test\", style=\"bold cyan\", justify=\"left\")\n",
    "check_table.add_column(\"Value\", justify=\"right\")\n",
    "check_table.add_column(\"Expected\", justify=\"right\")\n",
    "check_table.add_column(\"Match?\", justify=\"center\")\n",
    "\n",
    "check_table.add_row(\n",
    "    \"WOE Coefficient\",\n",
    "    f\"{woe_coef:.4f}\",\n",
    "    \"≈ 1.0 (WOE contains log odds)\",\n",
    "    \"v\" if abs(woe_coef - 1.0) < 0.01 else \"x\",\n",
    ")\n",
    "check_table.add_row(\n",
    "    \"WOE Intercept\",\n",
    "    f\"{woe_intercept:.4f}\",\n",
    "    f\"Prior log odds: {prior_log_odds:.4f}\",\n",
    "    \"v\" if intercept_match else \"x\",\n",
    ")\n",
    "\n",
    "panel = Panel(\n",
    "    check_table,\n",
    "    title=\"Verification of WOE Coefficient & Intercept\",\n",
    "    subtitle=\"Are WOE/logistic values as expected?\",\n",
    "    expand=False,\n",
    "    padding=(1, 8),\n",
    ")\n",
    "\n",
    "console.print(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4f092",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook has demonstrated several key relationships between Weight of Evidence (WOE) and standard errors:\n",
    "\n",
    "### Main Findings:\n",
    "\n",
    "1. **WOE is a centered version of log odds**: WOE = log odds - prior log odds\n",
    "2. **Standard errors are identical**: SE(WOE) = SE(log odds) because subtracting a constant doesn't change variance\n",
    "3. **Contingency table formulas work**: SE = √(1/n_bad + 1/n_good) matches logistic regression results\n",
    "4. **WOE regression has special properties**: coefficient ≈ 1.0, intercept ≈ prior log odds\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- WOE transformations preserve all statistical properties of log odds\n",
    "- Standard error calculations from contingency tables are valid for WOE\n",
    "- WOE-based models are mathematically equivalent to log odds models\n",
    "- The centering property of WOE doesn't affect inference or confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023db4bd",
   "metadata": {},
   "source": [
    "## 6. Simulation: Variance Equality of Log Odds and WOE\n",
    "\n",
    "This simulation demonstrates that **subtracting a constant (prior log odds) does not change variance**, which is why WOE and log odds have identical standard errors.\n",
    "\n",
    "### Mathematical Property:\n",
    "For any random variable X and constant c: Var(X - c) = Var(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration: Var(X - c) = Var(X)\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(1000)\n",
    "c = 5.0  # constant\n",
    "\n",
    "print(\"Variance Property Demonstration:\")\n",
    "print(f\"Var(X): {x.var():.6f}\")\n",
    "print(f\"Var(X - c): {(x - c).var():.6f}\")\n",
    "print(f\"Equal: {np.isclose(x.var(), (x - c).var())}\")\n",
    "print(\"\\nThis is why WOE and log odds have identical standard errors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b809a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical simulation with binary data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True probabilities from our example\n",
    "p1 = 30 / (30 + 20)  # Red group event rate\n",
    "prior_p = (30 + 10) / (30 + 10 + 20 + 40)  # Overall event rate\n",
    "\n",
    "# Sample sizes\n",
    "n_total = 50  # Group total size\n",
    "n_sim = 10_000  # Number of simulations\n",
    "\n",
    "# Store results\n",
    "log_odds_samples = []\n",
    "woe_samples = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    # Simulate binary outcomes for Red group\n",
    "    y = np.random.binomial(1, p1, size=n_total)\n",
    "    n_bad = y.sum()  # Number of bad outcomes\n",
    "    n_good = n_total - n_bad  # Number of good outcomes\n",
    "\n",
    "    # Compute log odds and WOE (avoid division by zero)\n",
    "    if n_bad > 0 and n_good > 0:\n",
    "        log_odds = np.log(n_bad / n_good)\n",
    "        log_odds_prior = np.log(prior_p / (1 - prior_p))\n",
    "        woe = log_odds - log_odds_prior\n",
    "\n",
    "        log_odds_samples.append(log_odds)\n",
    "        woe_samples.append(woe)\n",
    "\n",
    "# Convert to arrays\n",
    "log_odds_samples = np.array(log_odds_samples)\n",
    "woe_samples = np.array(woe_samples)\n",
    "\n",
    "# Compare empirical variances\n",
    "variance_log_odds = np.var(log_odds_samples, ddof=1)\n",
    "variance_woe = np.var(woe_samples, ddof=1)\n",
    "\n",
    "# Add rich table\n",
    "table = Table(title=\"Variance Comparison\", show_lines=True, min_width=60)\n",
    "table.add_column(\"Test\", style=\"bold cyan\", justify=\"left\")\n",
    "table.add_column(\"Value\", justify=\"right\")\n",
    "\n",
    "table.add_row(\"Variance of log odds\", f\"{variance_log_odds:.4f}\")\n",
    "table.add_row(\"Variance of WOE\", f\"{variance_woe:.4f}\")\n",
    "\n",
    "console.print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
